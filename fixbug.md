è¦å¸è½½ç°æœ‰çš„ PyTorch ç›¸å…³åŒ…å¹¶é‡æ–°å®‰è£…ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š

## 1. å¸è½½ç°æœ‰çš„ PyTorch ç›¸å…³åŒ…

```bash
pip uninstall torch torchvision torchaudio -y
```

## 2. æ¸…ç†ç¼“å­˜ï¼ˆå¯é€‰ä½†æ¨èï¼‰

```bash
pip cache purge
```

## 3. é‡æ–°å®‰è£… PyTorch nightly ç‰ˆæœ¬

```bash
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

## ä¸€è¡Œå‘½ä»¤å®Œæˆæ‰€æœ‰æ“ä½œ

```bash
pip uninstall torch torchvision torchaudio -y && pip cache purge && pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128

pip uninstall torch torchvision torchaudio -y && pip cache purge && pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu128

```

## éªŒè¯å®‰è£…

```bash
python3 -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}'); print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}'); print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')"
```

ä½¿ç”¨ `-y` å‚æ•°å¯ä»¥è‡ªåŠ¨ç¡®è®¤å¸è½½ï¼Œé¿å…äº¤äº’å¼æç¤ºã€‚



è¦å¸è½½ç°æœ‰çš„ flash-attn å¹¶å®‰è£…æ–°ç‰ˆæœ¬ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š

## 1. å¸è½½ç°æœ‰çš„ flash-attn

```bash
pip uninstall flash-attn -y
```

## 2. æ¸…ç†ç¼“å­˜ï¼ˆå¯é€‰ä½†æ¨èï¼‰

```bash
pip cache purge
```

## 3. å®‰è£…æ–°ç‰ˆæœ¬çš„ flash-attn

```bash
pip install flash_attn-2.8.0.post2+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
```

## ä¸€è¡Œå‘½ä»¤å®Œæˆæ‰€æœ‰æ“ä½œ

```bash
wget https://tenyunn-1354648220.cos.ap-guangzhou.myqcloud.com/flash_attn-2.8.0.post2%2Bcu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl && pip uninstall flash-attn -y && pip cache purge && pip install flash_attn-2.8.0.post2+cu12torch2.7cxx11abiTRUE-cp310-cp310-linux_x86_64.whl

wget https://tenyunn-1354648220.cos.ap-guangzhou.myqcloud.com/flash_attn-2.8.0.post2%2Bcu12torch2.7cxx11abiFALSE-cp310-cp310-linux_x86_64.whl && pip uninstall flash-attn -y && pip cache purge && pip install flash_attn-2.8.0.post2+cu12torch2.7cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
```

## å¦‚æœä½ æœ‰å…·ä½“çš„æ–°ç‰ˆæœ¬æ–‡ä»¶ï¼Œä¾‹å¦‚ï¼š

```bash
pip uninstall flash-attn -y && pip install /tmp/flash_attn-2.8.0.post2+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
```

## éªŒè¯å®‰è£…

```bash
python3 -c "import flash_attn; print('Flash Attention å®‰è£…æˆåŠŸ'); print(flash_attn.__version__)"
```

## æŸ¥çœ‹å½“å‰å®‰è£…çš„ç‰ˆæœ¬

```bash
pip show flash-attn
```

ä½¿ç”¨ `-y` å‚æ•°å¯ä»¥è‡ªåŠ¨ç¡®è®¤å¸è½½ï¼Œé¿å…äº¤äº’å¼æç¤ºã€‚



ğŸ“‹ å¿«é€Ÿå¼€å§‹:
  1. å¯åŠ¨ TyVLLM æœåŠ¡å™¨:
    nohup tyvllm-server --model /workspace/Qwen3-0.6B --port 8000 > tyvllm.log 2>&1 &
    nohup tyvllm-server --model /workspace/Qwen3-0.6B --tensor-parallel-size 2 --port 8000 > tyvllm.log 2>&1 &
  2. æŸ¥çœ‹æœåŠ¡æ—¥å¿—:
     tail -f tyvllm.log

  3. æµ‹è¯•æ¨ç†æ¥å£:
     python3 /workspace/ty-vllm-doc/examples/generate_stream.py --query '234 + 567'

  4. æŸ¥æ‰¾æœåŠ¡:
     pgrep -f tyvllm-server




docker pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/lmsysorg/sglang:blackwell
docker pull docker.m.daocloud.io/lmsysorg/sglang:blackwell




docker run -d --gpus '"device=0,1"' -p 2201:22 -p 8000:8888 -e TENYUNN_JUPYTER_TOKEN=324234141234134 -e TENYUNN_SSH_PWD=1234 --name pytorch_container_blackwell --cpus="8.0" --memory="16g" --memory-swap="32g" default-artifact.tencentcloudcr.com/public/tenyunn/base/pytorch-cuda12.8-ubuntu22.04:blackwell


æµ‹è¯•ç»“æœï¼š
å®¹å™¨ä¿¡æ¯:
   ç”¨æˆ·: root
   æ—¶é—´: 2025-07-26 11:07:31
   ä¸»æœº: 6cb2ee454b36
   ç³»ç»Ÿ: Linux 5.15.0-119-generic
   CPUæ ¸å¿ƒ: 8.0 cores
   å†…å­˜: 6.0Gi/16G (0.6%)

ç£ç›˜ä¿¡æ¯:
   ç³»ç»Ÿç›˜(/): 169G/3.5T (5%)
   æ•°æ®ç›˜(/datadisk): 169G/3.5T (5%)

GPUä¿¡æ¯:
   CUDAç‰ˆæœ¬: 12.8
   GPUæ•°é‡: 2
   GPUå‹å·: NVIDIA GeForce RTX 5090
PyTorchç‰ˆæœ¬: 2.7.1+cu128
CUDAå¯ç”¨: True
CUDAç‰ˆæœ¬: 12.8exit

docker run -d --gpus '"device=2,3"' -p 2202:22 -p 8001:8888 -e TENYUNN_JUPYTER_TOKEN=324234141234134 -e TENYUNN_SSH_PWD=1234 --name miniconda_container_blackwell --cpus="8.0" --memory="16g" --memory-swap="32g" default-artifact.tencentcloudcr.com/public/tenyunn/base/miniconda-cuda12.8-ubuntu22.04:blackwell


docker run -d --gpus '"device=4,5"' -p 2203:22 -p 8002:8888 -e TENYUNN_JUPYTER_TOKEN=324234141234134 -e TENYUNN_SSH_PWD=1234 --name tyvllm_miniconda_container_blackwell --cpus="8.0" --memory="16g" --memory-swap="32g" default-artifact.tencentcloudcr.com/public/tenyunn/base/tyvllm-miniconda-cuda12.8-ubuntu22.04:blackwell


docker run -d --gpus '"device=6,7"' -p 2204:22 -p 8003:8888 -e TENYUNN_JUPYTER_TOKEN=324234141234134 -e TENYUNN_SSH_PWD=1234 --name tyvllm_miniconda_container_blackwell --cpus="8.0" --memory="16g" --memory-swap="32g" default-artifact.tencentcloudcr.com/public/tenyunn/base/tyvllm-pytorch-cuda12.8-ubuntu22.04:blackwell


docker run -d --gpus all -p 2204:22 -p 8003:8888 -e TENYUNN_JUPYTER_TOKEN=324234141234134 -e TENYUNN_SSH_PWD=1234 --name tyvllm_miniconda_container_blackwell default-artifact.tencentcloudcr.com/public/tenyunn/base/tyvllm-pytorch-cuda12.8-ubuntu22.04:blackwell


python3 -c "import flash_attn; print('Flash Attention å®‰è£…æˆåŠŸ'); print(flash_attn.__version__)"

python3 -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}'); print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}'); print(f'CUDAç‰ˆæœ¬: {torch.version.cuda}')"


nohup tyvllm-server --model /workspace/Qwen3-0.6B --tensor-parallel-size 2 --port 8000 > tyvllm.log 2>&1 &
nohup tyvllm-server --model /workspace/Qwen3-0.6B --port 8000 > tyvllm.log 2>&1 &

python3 -c "import torch; print(f'å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}'); [print(f'GPU {i}: {torch.cuda.get_device_name(i)}') for i in range(torch.cuda.device_count())]"

DockerHubå¯ç”¨é•œåƒæºæ±‡æ€»
http://docker.m.daocloud.io
http://docker.imgdb.de
docker-0.unsee.tech
http://docker.hlmirror.com
http://cjie.eu.org


docker tag default-artifact.tencentcloudcr.com/public/tenyunn/base/pytorch-cuda12.8-ubuntu22.04:blackwell default-artifact.tencentcloudcr.com/public/tenyunn/base/pytorch-cuda12.8-ubuntu22.04:v0.0.4
default-artifact.tencentcloudcr.com/public/tenyunn/base/miniconda-cuda12.8-ubuntu22.04:blackwell
default-artifact.tencentcloudcr.com/public/tenyunn/base/tyvllm-pytorch-cuda12.8-ubuntu22.04:blackwell
default-artifact.tencentcloudcr.com/public/tenyunn/base/tyvllm-pytorch-cuda12.8-ubuntu22.04:blackwell



wget https://gh.llkk.cc/https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh